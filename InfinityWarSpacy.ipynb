{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=requests.get(\"https://transcripts.fandom.com/wiki/Avengers:_Infinity_War\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src=result.content\n",
    "soup=BeautifulSoup(src,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "s = []\n",
    "text = []\n",
    "for link in soup.find_all('div'):\n",
    "    if (\"id\",\"mw-content-text\") in link.attrs.items():\n",
    "        for l in link.find_all('p'):\n",
    "            if l.find(\"b\"):\n",
    "                author = re.findall(r'(.*?):', l.text)\n",
    "                text = re.sub(r'\\[.*?\\]',\"\",l.text)\n",
    "                text = re.sub(r'.*?:',\"\", text)\n",
    "                text = re.sub(r'\\n','',text)\n",
    "                try:\n",
    "                    s.append((author[0],text))\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "r = np.random.randint(100)\n",
    "\n",
    "#data=pd.DataFrame(s, columns = ['Title', 'Text'])[r:(r+10)]\n",
    "data=pd.DataFrame(s, columns = ['Title', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asgardian PA</td>\n",
       "      <td>This is the Asgardian refugee vessel Statesman. We are under assault, I repeat, we are under assault - The engines are dead, life support failing. Requesting aid from any vessel within range. We are 22 jump points out of Asgard.  Our crew is made up of Asgardian families, we have very few soldiers here. This is not a warcraft. I repeat, this is not a warcraft!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ebony Maw</td>\n",
       "      <td>Hear me, and rejoice. You have had the privilege of being saved by the Great Titan.... You may think this is suffering... no. it is salvation. Universal scales tip toward balance because of your sacrifice. Smile...  for even in death, you have become Children of Thanos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanos</td>\n",
       "      <td>I know what it's like to lose. To feel so desperately that you're right... yet to fail, nonetheless.  It's frightening. Turns the legs to jelly. I ask you, to what end? Dread it. Run from it. Destiny arrives all the same. And now, it's here. Or should I say... I AM.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thor</td>\n",
       "      <td>You talk too much.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanos</td>\n",
       "      <td>The Tesseract, or your brother's head. I assume you have a preference.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Title  \\\n",
       "0  Asgardian PA   \n",
       "1  Ebony Maw      \n",
       "2  Thanos         \n",
       "3  Thor           \n",
       "4  Thanos         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                          Text  \n",
       "0   This is the Asgardian refugee vessel Statesman. We are under assault, I repeat, we are under assault - The engines are dead, life support failing. Requesting aid from any vessel within range. We are 22 jump points out of Asgard.  Our crew is made up of Asgardian families, we have very few soldiers here. This is not a warcraft. I repeat, this is not a warcraft!  \n",
       "1   Hear me, and rejoice. You have had the privilege of being saved by the Great Titan.... You may think this is suffering... no. it is salvation. Universal scales tip toward balance because of your sacrifice. Smile...  for even in death, you have become Children of Thanos.                                                                                              \n",
       "2    I know what it's like to lose. To feel so desperately that you're right... yet to fail, nonetheless.  It's frightening. Turns the legs to jelly. I ask you, to what end? Dread it. Run from it. Destiny arrives all the same. And now, it's here. Or should I say... I AM.                                                                                                 \n",
       "3    You talk too much.                                                                                                                                                                                                                                                                                                                                                         \n",
       "4    The Tesseract, or your brother's head. I assume you have a preference.                                                                                                                                                                                                                                                                                                     "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=list(data2['Text'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' this is the asgardian refugee vessel statesman. we are under assault, i repeat, we are under assault - the engines are dead, life support failing. requesting aid from any vessel within range. we are 22 jump points out of asgard.\\xa0 our crew is made up of asgardian families, we have very few soldiers here. this is not a warcraft.\\xa0i repeat, this is not a warcraft!',\n",
       " ' hear me, and rejoice. you have had the privilege of being saved by the great titan.... you may think this is suffering... no. it is salvation. universal scales tip\\xa0toward balance because of your sacrifice. smile...  for even in death, you have become children of thanos.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create our list of punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=punc + '...' + '....'+''+'--'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create our list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = stop_words.union(set(stop_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the English parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hear me, and rejoice. you have had the privilege of being saved by the great titan.... you may think this is suffering... no. it is salvation. universal scales tip\\xa0toward balance because of your sacrifice. smile...  for even in death, you have become children of thanos.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating our tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens=parser(sentence)\n",
    "    \n",
    "    ## Lemmatize\n",
    "    mytokens=[word.lemma_.lower().strip() if word.lemma_!=\"-PRON-\" else word.lower_ for word in mytokens]\n",
    "    \n",
    "    ## Remove stop words\n",
    "    mytokens=[word for word in mytokens if word not in stop_words and word not in punc]\n",
    "    \n",
    "    ## return list of tokens\n",
    "    return mytokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_copy=text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[]\n",
    "for i in text:\n",
    "    tokens.append(spacy_tokenizer(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for sublist in tokens:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 15,3\n",
    "fdist.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_new=['stone','know','yes','come','...','think','right','time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text=[]\n",
    "for i in tokens:\n",
    "    new=[]\n",
    "    for j in i:\n",
    "        new.append(j)\n",
    "    final_text.append(' '.join(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_ = []\n",
    "for i in final_text:\n",
    "    final_text_.append(re.sub('\\d+',\"\", i))\n",
    "final_text_.append(i)\n",
    "final_text_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df = 0.5, stop_words = 'english', max_features= 1000, smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(final_text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated_SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors\n",
    "# Since we only have 170 features, take n_components = 100 (should be less than features)\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=100, algorithm='randomized', n_iter=100, random_state=123)\n",
    "\n",
    "svd_model.fit(X)\n",
    "svd_model.transform(X).shape\n",
    "#svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the weightages of individual terms in per topic / doc:\n",
    "(svd_model.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The components of svd_model are our topics, and we can access them using svd_model.components_.\n",
    "#Finally, let’s print a few most important words in each of the 21 topics and see how our model has done.\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, weight in enumerate(svd_model.components_):\n",
    "    terms_weight = zip(terms, weight)\n",
    "    #print(i)  i is the topic and svd_model has weight of each term in that topic\n",
    "    #print(list(terms_comp))\n",
    "    sorted_terms = sorted(terms_weight, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    y = []\n",
    "    for t in sorted_terms:\n",
    "        y.append(t[0])\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model for Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_lda = vectorizer.fit_transform(final_text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lda=pd.DataFrame(X_lda.toarray(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils,models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data_lda.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and \n",
    "#their respective location in the term-document matrix\n",
    "\n",
    "\n",
    "cv = vectorizer\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items()) ## we want location:term dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - \n",
    "#the number of topics and the number of passes\n",
    "\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, \n",
    "                      passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Verbs, Nouns and Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the verb Count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text:\n",
    "    doc1=nlp(i)\n",
    "    for i in doc1:\n",
    "        #print(i.pos_)\n",
    "        if i.is_stop:\n",
    "            continue\n",
    "        if i.pos_ == 'VERB':\n",
    "            if i.lemma_ in pos_count:\n",
    "                pos_count[i.lemma_] += 1\n",
    "            else:\n",
    "                pos_count[i.lemma_] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"top 10 VERBs {}\".format(sorted(pos_count.items(), key=lambda kv: kv[1], reverse=True)[:10]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count_verb={}\n",
    "pos_count_verb=sorted(pos_count.items(), key=lambda kv: kv[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(tup, di): \n",
    "    di=dict(tup)\n",
    "    return di "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary={}\n",
    "pos_count_verb=Convert(pos_count_verb, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distribution of Verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.bar(range(len(pos_count_verb)), list(pos_count_verb.values()), align='center')\n",
    "plt.xticks(range(len(pos_count_verb)), list(pos_count_verb.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Adverbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverb_count={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in text:\n",
    "    doc1=nlp(i)\n",
    "    for i in doc1:\n",
    "        #print(i.pos_)\n",
    "        if i.is_stop:\n",
    "            continue\n",
    "        if i.pos_ == 'ADV':\n",
    "                try:\n",
    "                    if i.lemma_ in adverb_count:\n",
    "                        adverb_count[i.lemma_] += 1\n",
    "                    else:\n",
    "                        adverb_count[i.lemma_] = 1 \n",
    "                except:\n",
    "                    print(i.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"top 10 ADVERBs {}\".format(sorted(adverb_count.items(), key=lambda kv: kv[1], reverse=True)[:10]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverb_count_new={}\n",
    "adverb_count_new=sorted(adverb_count.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "def Convert(tup, di): \n",
    "    di=dict(tup)\n",
    "    return di "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary={}\n",
    "adverb_count=Convert(adverb_count_new, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(range(len(adverb_count)), list(adverb_count.values()), align='center')\n",
    "plt.xticks(range(len(adverb_count)), list(adverb_count.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_count={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in text:\n",
    "    doc1=nlp(i)\n",
    "    for i in doc1:\n",
    "        #print(i.pos_)\n",
    "        if i.is_stop:\n",
    "            continue\n",
    "        if i.pos_ == 'NOUN':\n",
    "                try:\n",
    "                    if i.lemma_ in noun_count:\n",
    "                        noun_count[i.lemma_] += 1\n",
    "                    else:\n",
    "                        noun_count[i.lemma_] = 1 \n",
    "                except:\n",
    "                    print(i.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"top 10 Nouns {}\".format(sorted(noun_count.items(), key=lambda kv: kv[1], reverse=True)[:10]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_count_new={}\n",
    "noun_count_new=sorted(noun_count.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "def Convert(tup, di): \n",
    "    di=dict(tup)\n",
    "    return di "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary={}\n",
    "noun_count=Convert(noun_count_new, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(range(len(noun_count)), list(noun_count.values()), align='center')\n",
    "plt.xticks(range(len(noun_count)), list(noun_count.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjective_count={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in text:\n",
    "    doc1=nlp(i)\n",
    "    for i in doc1:\n",
    "        #print(i.pos_)\n",
    "        if i.is_stop:\n",
    "            continue\n",
    "        if i.pos_ == 'ADJ':\n",
    "                try:\n",
    "                    if i.lemma_ in adjective_count:\n",
    "                        adjective_count[i.lemma_] += 1\n",
    "                    else:\n",
    "                        adjective_count[i.lemma_] = 1 \n",
    "                except:\n",
    "                    print(i.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"top 10 Adjectives {}\".format(sorted(adjective_count.items(), key=lambda kv: kv[1], reverse=True)[:10]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjective_count_new={}\n",
    "adjective_count_new=sorted(adjective_count.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "def Convert(tup, di): \n",
    "    di=dict(tup)\n",
    "    return di "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary={}\n",
    "adjective_count=Convert(adjective_count_new, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(range(len(adjective_count)), list(adjective_count.values()), align='center')\n",
    "plt.xticks(range(len(adjective_count)), list(adjective_count.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity wise Verbs, Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 30 Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {}\n",
    "for i in text:\n",
    "    doc1=nlp(i)\n",
    "# named entities\n",
    "    for ent in doc1.ents:\n",
    "        #if(ent.label_ =='PERSON'):\n",
    "    # Print the entity text and its label\n",
    "        if ent.text in entities:\n",
    "            entities[ent.text] += 1\n",
    "        else:\n",
    "            entities[ent.text] = 1\n",
    "        \n",
    "print(\"top entities {}\".format(sorted(entities.items(),\n",
    "      key=lambda kv: kv[1], reverse=True)[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_new={}\n",
    "entities_new=sorted(entities.items(), key=lambda kv: kv[1], reverse=True)[:30]\n",
    "def Convert(tup, di): \n",
    "    di=dict(tup)\n",
    "    return di "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary={}\n",
    "entities=Convert(entities_new, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,20))\n",
    "plt.bar(range(len(entities)), list(entities.values()), align='center')\n",
    "plt.xticks(range(len(entities)), list(entities.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity wise Verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbs by Thanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "thanos_verb={}\n",
    "for j in text:\n",
    "    doc1=nlp(j)\n",
    "    for ent in doc1:\n",
    "        if(ent.text=='Thanos'):\n",
    "            for i in doc1:\n",
    "                #print(i.pos_)\n",
    "                if i.is_stop:\n",
    "                    continue\n",
    "                if i.pos_ == 'VERB':\n",
    "                    if i.lemma_ in thanos_verb:\n",
    "                        thanos_verb[i.lemma_] += 1\n",
    "                    else:\n",
    "                        thanos_verb[i.lemma_] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 Verbs [('know', 8), ('kill', 7), ('go', 6), ('get', 5), ('stop', 4), ('come', 3), ('need', 3), ('talk', 2), ('steal', 2), ('see', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(\"top 10 Verbs {}\".format(sorted(thanos_verb.items(), key=lambda kv: kv[1], reverse=True)[:10]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nouns by Thanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "thanos_noun={}\n",
    "for i in text:\n",
    "    doc1=nlp(i)\n",
    "    for ent in doc1.ents:\n",
    "        if(ent.text=='Thanos'):\n",
    "            for i in doc1:\n",
    "                #print(i.pos_)\n",
    "                if i.is_stop:\n",
    "                    continue\n",
    "                if i.pos_ == 'NOUN':\n",
    "                    if i.lemma_ in thanos_noun:\n",
    "                        thanos_noun[i.lemma_] += 1\n",
    "                    else:\n",
    "                        thanos_noun[i.lemma_] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 Adjectives [('stone', 5), ('daughter', 3), ('way', 3), ('experience', 2), ('ship', 2), ('moron', 2), ('army', 2), ('life', 2), ('head', 2), ('year', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(\"top 10 Nouns {}\".format(sorted(thanos_noun.items(), key=lambda kv: kv[1], reverse=True)[:10]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tony Stark Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stark_verb={}\n",
    "for i in text:\n",
    "    doc1=nlp(i)\n",
    "    for ent in doc1.ents:\n",
    "        if(ent.text=='Stark'):\n",
    "            for i in doc1:\n",
    "                #print(i.pos_)\n",
    "                if i.is_stop:\n",
    "                    continue\n",
    "                if i.pos_ == 'VERB':\n",
    "                    if i.lemma_ in Stark_verb:\n",
    "                        Stark_verb[i.lemma_] += 1\n",
    "                    else:\n",
    "                        Stark_verb[i.lemma_] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 Verbs [('wanna', 4), ('know', 2), ('beam', 1), ('smell', 1), ('give', 1), ('understand', 1), ('come', 1), ('save', 1), ('hesitate', 1), ('let', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"top 10 Verbs {}\".format(sorted(Stark_verb.items(), key=lambda kv: kv[1], reverse=True)[:10]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stark_noun={}\n",
    "for i in text:\n",
    "    doc1=nlp(i)\n",
    "    for ent in doc1.ents:\n",
    "        if(ent.text=='Tony'):\n",
    "            for i in doc1:\n",
    "                #print(i.pos_)\n",
    "                if i.is_stop:\n",
    "                    continue\n",
    "                if i.pos_ == 'NOUN':\n",
    "                    if i.lemma_ in Stark_noun:\n",
    "                        Stark_noun[i.lemma_] += 1\n",
    "                    else:\n",
    "                        Stark_noun[i.lemma_] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 Verbs [('plague', 1), ('planet', 1), ('population', 1), ('attack', 1), ('telling', 1), ('creature', 1), ('universe', 1), ('hand', 1), ('bot', 1), ('choice', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"top 10 Verbs {}\".format(sorted(Stark_noun.items(), key=lambda kv: kv[1], reverse=True)[:10]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "Groot_noun={}\n",
    "for i in text:\n",
    "    doc1=nlp(i)\n",
    "    for ent in doc1.ents:\n",
    "        if(ent.text=='Groot'):\n",
    "            for i in doc1:\n",
    "                #print(i.pos_)\n",
    "                if i.is_stop:\n",
    "                    continue\n",
    "                if i.pos_ == 'NOUN':\n",
    "                    if i.lemma_ in Groot_noun:\n",
    "                        Groot_noun[i.lemma_] += 1\n",
    "                    else:\n",
    "                        Groot_noun[i.lemma_] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 Nouns [('face', 2), ('thing', 2), ('moron', 2), ('ship', 1), ('assortment', 1), ('pirate', 1), ('angel', 1), ('way', 1), ('captain', 1), ('game', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"top 10 Nouns {}\".format(sorted(Groot_noun.items(), key=lambda kv: kv[1], reverse=True)[:10]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
